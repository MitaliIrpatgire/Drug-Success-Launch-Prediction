# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dhudaXlUh0SlSPZ3qfuhRKARzoLd4AXS
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.utils import resample

!pip install gdown

!gdown --id 16RTyZuZ9_APwggQhZ34vJVf8YibhLwwY -O drug_launch_data.csv

import pandas as pd

# Step 1: Load the dataset
url = 'drug_launch_data.csv'  # Local file path to the dataset
data = pd.read_csv(url)

# Show the first few rows of the dataset to verify
data.head()

!pip install shap flask

import shap

# Step 1: Load the dataset
url = '/content/drug_launch_data.csv'  # Link to your dataset
data = pd.read_csv(url)

# Step 2: Exploratory Data Analysis (EDA)
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n", data.info())

# Checking missing values
print("\nMissing Values:\n", data.isnull().sum())

# Drop 'Drug Name' and one-hot encode 'Marketed Region' or other categorical columns
numeric_data = pd.get_dummies(data.drop(['Drug Name'], axis=1), drop_first=True)

# Plotting the correlation matrix to understand feature relationships
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Print column names to verify the existence of 'Success' column
print("Column names in dataset:", data.columns)

# Ensure that the 'Success' column exists and is named correctly
if 'Success ' in data.columns:
    # Step 3: Handling Imbalanced Data
    # Separate majority and minority classes if imbalanced
    majority_class = data[data['Success '] == 1]
    minority_class = data[data['Success '] == 0]
else:
    print("Error: 'Success' column not found in the dataset. Please check the column name.")

# Check if 'Success' column exists
if 'Success ' in data.columns:
    # Step 3: Handling Imbalanced Data
    # Separate majority and minority classes if imbalanced (Success = 1 and Success = 0)
    majority_class = data[data['Success '] == 1]
    minority_class = data[data['Success '] == 0]

    # Upsample the minority class
    minority_upsampled = resample(minority_class,
                                  replace=True,    # sample with replacement
                                  n_samples=len(majority_class),  # to match majority class
                                  random_state=42)  # reproducible results

    # Combine majority and upsampled minority class
    data_upsampled = pd.concat([majority_class, minority_upsampled])

    print("Upsampling completed successfully.")
else:
    print("Error: 'Success' column not found in the dataset. Please check the column name.")

# Step 4: Data Preprocessing
# Convert categorical data (e.g., Marketed Region) into numerical values using one-hot encoding
data_upsampled = pd.get_dummies(data_upsampled, columns=['Marketed Region'], drop_first=True)

# Separate the features and the target variable (Success)
X = data_upsampled.drop(['Drug Name', 'Success '], axis=1)
y = data_upsampled['Success ']

# Split the dataset into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#print("Unique values in y_test:", np.unique(y_test, return_counts=True))
from sklearn.model_selection import train_test_split

# Ensure the split maintains the class distribution
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Verify that y_test now contains both classes
print("Unique values in y_test after stratification:", np.unique(y_test, return_counts=True))


# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)



# Step 5: Hyperparameter Tuning for Random Forest
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Adjust cv to 2 for smaller datasets
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                           param_grid=param_grid, cv=2, n_jobs=-1, verbose=2)

# Train the model
grid_search.fit(X_train, y_train)

# Best Parameters
print(f"Best Parameters: {grid_search.best_params_}")

# Step 6: Model Building with Best Parameters
model = RandomForestClassifier(**grid_search.best_params_)
model.fit(X_train, y_train)

# Model Prediction
y_pred = model.predict(X_test)

# Step 7: Model Evaluation

# Accuracy and Classification Report
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

# Check the class distribution in y_test
import numpy as np
print("Unique values in y_test:", np.unique(y_test, return_counts=True))

# ROC-AUC Curve
y_pred_prob = model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_pred_prob)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Step 8: Feature Importance
importances = model.feature_importances_
feature_names = X.columns
sorted_indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.bar(range(X.shape[1]), importances[sorted_indices], align="center")
plt.xticks(range(X.shape[1]), feature_names[sorted_indices], rotation=90)
plt.tight_layout()
plt.show()

# Step 9: Model Explainability using SHAP

# Initialize the SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

import numpy as np

# Manually specify or generate feature names if X_test is a NumPy array
if isinstance(X_test, np.ndarray):
    feature_names = [f"Feature {i+1}" for i in range(X_test.shape[1])]
else:
    feature_names = X_test.columns

# Print the original shapes of X_test and shap_values for debugging
print(f"Original Shape of X_test: {X_test.shape}")
print(f"Original Shape of shap_values[0]: {shap_values[0].shape}")

# Transpose shap_values[0] to match the shape of X_test if needed
if shap_values[0].shape != X_test.shape:
    shap_values_transposed = shap_values[0].T
    print(f"Transposed Shape of shap_values[0]: {shap_values_transposed.shape}")
else:
    shap_values_transposed = shap_values[0]

# Ensure SHAP values and X_test are aligned in shape
if X_test.shape[1] == shap_values_transposed.shape[1]:
    # Visualize SHAP summary plot with generated feature names
    shap.summary_plot(shap_values_transposed, X_test, feature_names=feature_names)
else:
    print("There is still a shape mismatch. Please check how the data is being prepared.")

# Assuming binary classification or a model with multiple outputs
instance_idx = 0  # Example for the first instance

# Print the shapes of SHAP values and X_test for debugging
print(f"Shape of shap_values[1][instance_idx]: {shap_values[1][instance_idx].shape}")
print(f"Shape of X_test[instance_idx]: {X_test[instance_idx].shape}")

# Manually specify feature names since X_test is a NumPy array
feature_names = [f"Feature {i+1}" for i in range(X_test.shape[1])]

# Ensure that the number of SHAP values matches the number of features
if len(shap_values[1][instance_idx]) == len(X_test[instance_idx]):
    # Visualize SHAP force plot for the selected instance
    shap.force_plot(explainer.expected_value[1], shap_values[1][instance_idx], X_test[instance_idx], feature_names=feature_names)
else:
    print("There is a mismatch between the number of features and SHAP values.")

!pip install flask-ngrok

import pickle

# Save the trained model to a file
with open('model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

# Save the scaler to a file
with open('scaler.pkl', 'wb') as scaler_file:
    pickle.dump(scaler, scaler_file)

# Load the model from the file
with open('model.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

# Load the scaler from the file
with open('scaler.pkl', 'rb') as scaler_file:
    scaler = pickle.load(scaler_file)

from google.colab import files
files.download('model.pkl')
files.download('scaler.pkl')

!pkill ngrok  # Kill any running ngrok processes

!pip install pyngrok

from pyngrok import ngrok

# Replace this with your actual ngrok auth token
!ngrok authtoken '2Qodf2Thy7FOVCfcYyi4ZFPkksp_4d1LvTP4gEYAsabkBd8ej'

!pip install bottle

!lsof -i :8080
!kill -9 <PID>

from bottle import Bottle, run, request, response
import pickle
import numpy as np
import os

# Initialize Bottle app
app = Bottle()

# Load pre-trained model and scaler
model = pickle.load(open('model.pkl', 'rb'))
scaler = pickle.load(open('scaler.pkl', 'rb'))

# Prediction route
@app.post('/predict')
def predict():
    try:
        data = request.json
        # Extract features
        features = np.array([data['R&D Spend'], data['Trial Duration'], data['Competitor Drugs'],
                             data['Target Market Size'], data['Marketing Budget'], data['Marketed Region']])
        # Scale features
        scaled_features = scaler.transform([features])

        # Make prediction
        prediction = model.predict(scaled_features)
        success_prob = model.predict_proba(scaled_features)[0][1]

        # Return result
        return {
            "prediction": "Success" if prediction[0] == 1 else "Failure",
            "success_probability": success_prob
        }
    except Exception as e:
        response.status = 500
        return {"error": str(e)}

# Get the port from the environment, default to 8080
port = int(os.environ.get("PORT", 1010))

# Run the Bottle app on the Vercel-assigned port
run(app, host='0.0.0.0', port=port)

!pip install --ignore-installed blinker

!pip install "apache-airflow[celery]==2.1.2" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.1.2/constraints-3.7.txt"

!pip install "apache-airflow[celery]==2.1.2" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.1.2/constraints-3.7.txt"

!pip install apache-airflow



# Step 12: Automating the Data Pipeline with Airflow

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import pickle

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 9, 25),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG('drug_launch_retraining', default_args=default_args, schedule_interval='@weekly')

# Define task: Load new data
def fetch_new_data():
    # Simulate new data loading (this would be a connection to a DB or an API call in real world)
    new_data = pd.read_csv('/path/to/new_data.csv')
    return new_data

# Define task: Retrain model
def retrain_model(**kwargs):
    # Fetch new data
    ti = kwargs['ti']
    new_data = ti.xcom_pull(task_ids='fetch_new_data')

    # Feature engineering
    # Preprocess and retrain the model
    X = new_data.drop(['Drug Name', 'Success'], axis=1)
    y = new_data['Success']

    # Split and retrain
    model.fit(X, y)

    # Save updated model
    with open('model.pkl', 'wb') as f:
        pickle.dump(model, f)

# Define Airflow tasks
fetch_data_task = PythonOperator(task_id='fetch_new_data', python_callable=fetch_new_data, dag=dag)
retrain_task = PythonOperator(task_id='retrain_model', python_callable=retrain_model, provide_context=True, dag=dag)

# Task dependencies
fetch_data_task >> retrain_task

